name: Test and Deploy DAGs

on:
  push:
    branches:
      - main
    paths:
      - 'corporate_dag_contrbiapdatainfra/**'  # Trigger only when DAG-related files are pushed

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.9

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt  # Ensure pytest and other dependencies are installed

      - name: Run Pytest tests
        run: |
          pytest tests/test_dag.py  # Ensure your test file path is correct

  deploy:
    runs-on: ubuntu-latest
    needs: test  # Only deploy if tests pass
    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Authenticate with Google Cloud
        run: |
          # Set up the Google Cloud service account for authentication
          echo "${{ secrets.GCP_SA_KEY }}" > /tmp/gcp-key.json
          gcloud auth activate-service-account --key-file=/tmp/gcp-key.json
          gcloud config set project biap-datainfra-gcp

      - name: Copy DAGs to Airflow VM
        run: |
          # Use gcloud to copy the updated DAGs from GitHub to the VM's Airflow DAGs directory
          gcloud compute scp --recurse corporate_dag_contrbiapdatainfra/dags/* contrbiapdatainfra:/home/corporate/dags/ --zone=asia-southeast2-b --project YOUR_PROJECT_ID

      - name: Restart Airflow Scheduler
        run: |
          # SSH into the Airflow VM to restart the Airflow scheduler
          gcloud compute ssh contrbiapdatainfra --zone=asia-southeast2-b --project biap-datainfra-gcp --command "sudo pkill -f airflow-scheduler"
          gcloud compute ssh contrbiapdatainfra --zone=asia-southeast2-b --project biap-datainfra-gcp --command "airflow scheduler &"
